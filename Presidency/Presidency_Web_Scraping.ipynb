{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shreyash-prog/U.S-Presidential-Speech-Analysis/blob/main/Presidency/Presidency_Web_Scraping.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "lNMD8KmVhROC",
        "outputId": "4170a905-348c-4c18-9076-67bed721c95d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Enter the base url of the website:https://www.presidency.ucsb.edu/documents/app-categories/elections-and-transitions/campaign-documents?items_per_page=60\n",
            "Enter the number of pages for which you want to extract the data:1\n",
            "Data written to output.txt\n",
            "Data written to output.txt\n",
            "Data written to output.txt\n",
            "Data written to output.txt\n",
            "Data written to output.txt\n",
            "Data written to output.txt\n",
            "Data written to output.txt\n",
            "Data written to output.txt\n",
            "Data written to output.txt\n",
            "Data written to output.txt\n",
            "Data written to output.txt\n",
            "Data written to output.txt\n",
            "Data written to output.txt\n",
            "Data written to output.txt\n",
            "Data written to output.txt\n",
            "Data written to output.txt\n",
            "Data written to output.txt\n",
            "Data written to output.txt\n",
            "Data written to output.txt\n",
            "Data written to output.txt\n",
            "Data written to output.txt\n",
            "Data written to output.txt\n",
            "Data written to output.txt\n",
            "Data written to output.txt\n",
            "Data written to output.txt\n",
            "Data written to output.txt\n",
            "Data written to output.txt\n",
            "Data written to output.txt\n",
            "Data written to output.txt\n",
            "Data written to output.txt\n",
            "Data written to output.txt\n",
            "Data written to output.txt\n",
            "Data written to output.txt\n",
            "Data written to output.txt\n",
            "Data written to output.txt\n",
            "Data written to output.txt\n",
            "Data written to output.txt\n",
            "Data written to output.txt\n",
            "Data written to output.txt\n",
            "Data written to output.txt\n",
            "Data written to output.txt\n",
            "Data written to output.txt\n",
            "Data written to output.txt\n",
            "Data written to output.txt\n",
            "Data written to output.txt\n",
            "Data written to output.txt\n",
            "Data written to output.txt\n",
            "Data written to output.txt\n",
            "Data written to output.txt\n",
            "Data written to output.txt\n",
            "Data written to output.txt\n",
            "Data written to output.txt\n",
            "Data written to output.txt\n",
            "Data written to output.txt\n",
            "Data written to output.txt\n",
            "Data written to output.txt\n",
            "Data written to output.txt\n",
            "Data written to output.txt\n",
            "Data written to output.txt\n",
            "Data written to output.txt\n",
            "Data written to output.txt\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_55f17eee-66c4-41db-9f04-338a9dab6ce4\", \"scraped_data.csv\", 380387)"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "#Code to scrape the first (n) pages\n",
        "\n",
        "# Import necessary libraries\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "from google.colab import files\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Function to scrape the main website for speech links and dates\n",
        "def scrape_main_website(url):\n",
        "    # Initialize empty lists to store URLs and dates\n",
        "    url_list = []#List of the URLs on the page\n",
        "    date_list = []#List of all dates\n",
        "\n",
        "    # Make a GET request to the provided URL\n",
        "    response = requests.get(url)\n",
        "\n",
        "    # Check if the request was successful (status code 200)\n",
        "    if response.status_code == 200:\n",
        "        # Parse the HTML content\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "        # Extract URLs from anchor tags (links)\n",
        "        paragraphs = soup.find_all('p')\n",
        "        data = '\\n'.join([p.get_text() for p in paragraphs])\n",
        "\n",
        "        links = soup.find_all('a', href=True)\n",
        "\n",
        "        # Print or process the URLs\n",
        "        if links:\n",
        "            for link in links:\n",
        "                url = link['href']\n",
        "                # Filter URLs to include only speech-related links\n",
        "                if '/documents/' in url:\n",
        "                    if 'presidential-documents-archive-guidebook' not in url and 'category-attributes' not in url and 'presidential-documents-archive-guidebook' not in url and 'app-categories' not in url:\n",
        "                        url_list.append('https://www.presidency.ucsb.edu'+url)\n",
        "\n",
        "        # Extract dates from span tags with class 'date-display-single'\n",
        "        dates = soup.find_all('span', class_='date-display-single')\n",
        "        if dates:\n",
        "            for date in dates:\n",
        "                date_list.append(date.text)\n",
        "\n",
        "        # Return scraped data\n",
        "        return data, url_list, date_list\n",
        "    else:\n",
        "        print(f\"Failed to fetch the page. Status code: {response.status_code}\")\n",
        "        return None\n",
        "\n",
        "# Function to write data to a file\n",
        "def write_to_file(data, filename):\n",
        "    with open(filename, 'w', encoding='utf-8') as file:\n",
        "        file.write(data)\n",
        "    print(f\"Data written to {filename}\")\n",
        "\n",
        "# Function to scrape content from individual speech pages\n",
        "def scrape_content_website(url):\n",
        "    # Make a GET request to the provided URL\n",
        "    response = requests.get(url)\n",
        "\n",
        "    # Check if the request was successful (status code 200)\n",
        "    if response.status_code == 200:\n",
        "        # Parse the HTML content\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "        # Extract text from paragraph tags (speech content)\n",
        "        paragraphs = soup.find_all('p')\n",
        "        data = '\\n'.join([p.get_text() for p in paragraphs])\n",
        "\n",
        "        # Return scraped speech content\n",
        "        return data\n",
        "    else:\n",
        "        print(f\"Failed to fetch the page. Status code: {response.status_code}\")\n",
        "        return None\n",
        "\n",
        "# Function to perform scraping across multiple pages\n",
        "def scraping(n, base_url):\n",
        "    number_of_pages_to_scrape = n\n",
        "    # Extract number of results per page from the base URL\n",
        "    num_results_per_page = int(base_url[len(base_url)-2:len(base_url)])\n",
        "\n",
        "    # Initialize empty lists to store final scraped data\n",
        "    final_date_list, final_related_lst, final_title_lst, final_url_list, final_content_lst = [],[],[],[],[]\n",
        "\n",
        "    #Check if number of pages to be scraped is equal to 1\n",
        "    if number_of_pages_to_scrape == 1:\n",
        "        url_to_scrape = base_url\n",
        "        date_list, related_lst, title_lst, url_list, content_lst = scrape(url_to_scrape, num_results_per_page)\n",
        "        final_date_list, final_related_lst, final_title_lst, final_url_list, final_content_lst = date_list, related_lst, title_lst, url_list, content_lst\n",
        "\n",
        "    #Check if number of pages to be scraped is greater than 1\n",
        "    elif number_of_pages_to_scrape > 1:\n",
        "        for page_num in range(1, number_of_pages_to_scrape+1):\n",
        "            url_to_scrape = f'{base_url}&page={page_num}'\n",
        "            date_list, related_lst, title_lst, url_list, content_lst = scrape(url_to_scrape, num_results_per_page)\n",
        "            final_date_list+=date_list\n",
        "            final_related_lst+=related_lst\n",
        "            final_title_lst+=title_lst\n",
        "            final_url_list+=url_list\n",
        "            final_content_lst+=content_lst\n",
        "\n",
        "    #Handle the case in which a number lesser than 1 is given by the user\n",
        "    else:\n",
        "        print('Error: Please provide a number greater than or equal to 1')\n",
        "\n",
        "    return final_date_list, final_related_lst, final_title_lst, final_url_list, final_content_lst\n",
        "\n",
        "def scrape(url_to_scrape, num_results_per_page):\n",
        "    output_filename = 'output.txt'\n",
        "\n",
        "    scraped_data, url_list, date_list = scrape_main_website(url_to_scrape)\n",
        "\n",
        "    if scraped_data:\n",
        "        write_to_file(scraped_data, output_filename)\n",
        "\n",
        "    lst=[]\n",
        "    try:\n",
        "        with open(output_filename, 'r', encoding='utf-8') as read_file:\n",
        "            for line in read_file:\n",
        "                lst.append(line.strip())\n",
        "    except FileNotFoundError:\n",
        "        print(f\"The file '{file_path}' does not exist.\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "    lst=lst[2:len(lst)-3]\n",
        "    title_lst = [lst[i] for i in range(len(lst)) if i%2==0] #List of all titles on the page\n",
        "    related_lst = [lst[i] for i in range(len(lst)) if i%2!=0] #List of the related persons\n",
        "\n",
        "    content_lst = []# List of speech content for the page - for all links\n",
        "    for url in url_list:\n",
        "        scraped_speech_data = scrape_content_website(url)\n",
        "\n",
        "        if scraped_data:\n",
        "            write_to_file(scraped_speech_data, output_filename)\n",
        "\n",
        "        content_str=''\n",
        "        try:\n",
        "            with open(output_filename, 'r', encoding='utf-8') as read_file:\n",
        "                for line in read_file:\n",
        "                    if 'The American Presidency Project' not in line and 'Twitter Facebook' not in line:\n",
        "                        content_str+=line\n",
        "        except FileNotFoundError:\n",
        "            print(f\"The file '{file_path}' does not exist.\")\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred: {e}\")\n",
        "\n",
        "        content_lst.append(content_str)\n",
        "\n",
        "    return date_list[:num_results_per_page], related_lst, title_lst, url_list[:num_results_per_page], content_lst[:num_results_per_page]\n",
        "\n",
        "\n",
        "#User input\n",
        "url = input('Enter the base url of the website:')\n",
        "num_of_pages = int(input('Enter the number of pages for which you want to extract the data:'))\n",
        "date_list, related_lst, title_lst, url_list, content_lst = scraping(num_of_pages, url)\n",
        "\n",
        "data = {\n",
        "    'Speech Link': url_list,\n",
        "    'Date of Speech': date_list,\n",
        "    'Speech Title': title_lst,\n",
        "    'Related Person': related_lst,\n",
        "    'Speech Content': content_lst\n",
        "}\n",
        "\n",
        "scraped_data_df = pd.DataFrame(data)\n",
        "\n",
        "#Uncomment to store the DataFrame to a CSV file\n",
        "scraped_data_df.to_csv('/Users/shreyashkalal/Desktop/scraped_data.csv', index=False)\n",
        "files.download('/content/drive/My Drive/scraped_data.csv')\n",
        "\n",
        "#Uncomment to store the DataFrame to a Feather file\n",
        "#A Feather file is more effecient as compared to a CSV file in terms of memory and time\n",
        "#feather_file_path = 'scraped_data.feather'\n",
        "#scraped_data_df.to_feather(feather_file_path)\n",
        "# files.download('/content/drive/My Drive/scraped_data.feather')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mj9HEWi7MjQ6"
      },
      "outputs": [],
      "source": [
        "#Code to scrape date from specific page intervals\n",
        "\n",
        "# Import necessary libraries\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "from google.colab import files\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Function to scrape the main website for speech links and dates\n",
        "def scrape_main_website(url):\n",
        "    # Initialize empty lists to store URLs and dates\n",
        "    url_list = []#List of the URLs on the page\n",
        "    date_list = []#List of all dates\n",
        "\n",
        "    # Make a GET request to the provided URL\n",
        "    response = requests.get(url)\n",
        "\n",
        "    # Check if the request was successful (status code 200)\n",
        "    if response.status_code == 200:\n",
        "        # Parse the HTML content\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "        # Extract URLs from anchor tags (links)\n",
        "        paragraphs = soup.find_all('p')\n",
        "        data = '\\n'.join([p.get_text() for p in paragraphs])\n",
        "\n",
        "        links = soup.find_all('a', href=True)\n",
        "\n",
        "        # Print or process the URLs\n",
        "        if links:\n",
        "            for link in links:\n",
        "                url = link['href']\n",
        "                # Filter URLs to include only speech-related links\n",
        "                if '/documents/' in url:\n",
        "                    if 'presidential-documents-archive-guidebook' not in url and 'category-attributes' not in url and 'presidential-documents-archive-guidebook' not in url and 'app-categories' not in url:\n",
        "                        url_list.append('https://www.presidency.ucsb.edu'+url)\n",
        "\n",
        "        # Extract dates from span tags with class 'date-display-single'\n",
        "        dates = soup.find_all('span', class_='date-display-single')\n",
        "        if dates:\n",
        "            for date in dates:\n",
        "                date_list.append(date.text)\n",
        "\n",
        "        # Return scraped data\n",
        "        return data, url_list, date_list\n",
        "    else:\n",
        "        print(f\"Failed to fetch the page. Status code: {response.status_code}\")\n",
        "        return None\n",
        "\n",
        "# Function to write data to a file\n",
        "def write_to_file(data, filename):\n",
        "    with open(filename, 'w', encoding='utf-8') as file:\n",
        "        file.write(data)\n",
        "    print(f\"Data written to {filename}\")\n",
        "\n",
        "# Function to scrape content from individual speech pages\n",
        "def scrape_content_website(url):\n",
        "    # Make a GET request to the provided URL\n",
        "    response = requests.get(url)\n",
        "\n",
        "    # Check if the request was successful (status code 200)\n",
        "    if response.status_code == 200:\n",
        "        # Parse the HTML content\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "        # Extract text from paragraph tags (speech content)\n",
        "        paragraphs = soup.find_all('p')\n",
        "        data = '\\n'.join([p.get_text() for p in paragraphs])\n",
        "\n",
        "        # Return scraped speech content\n",
        "        return data\n",
        "    else:\n",
        "        print(f\"Failed to fetch the page. Status code: {response.status_code}\")\n",
        "        return None\n",
        "\n",
        "# Function to perform scraping across multiple pages\n",
        "def scraping(start_page, end_page, base_url):\n",
        "    # Extract number of results per page from the base URL\n",
        "    num_results_per_page = int(base_url[len(base_url)-2:len(base_url)])\n",
        "\n",
        "    # Initialize empty lists to store final scraped data\n",
        "    final_date_list, final_related_lst, final_title_lst, final_url_list, final_content_lst = [],[],[],[],[]\n",
        "\n",
        "    # Check if start_page is equal to end_page\n",
        "    if start_page == end_page:\n",
        "        url_to_scrape = base_url\n",
        "        date_list, related_lst, title_lst, url_list, content_lst = scrape(url_to_scrape, num_results_per_page)\n",
        "        final_date_list, final_related_lst, final_title_lst, final_url_list, final_content_lst = date_list, related_lst, title_lst, url_list, content_lst\n",
        "\n",
        "    # Check if end_page is greater than start_page\n",
        "    elif end_page > start_page:\n",
        "        for page_num in range(start_page, end_page+1):\n",
        "            url_to_scrape = f'{base_url}&page={page_num}'\n",
        "            date_list, related_lst, title_lst, url_list, content_lst = scrape(url_to_scrape, num_results_per_page)\n",
        "            final_date_list+=date_list\n",
        "            final_related_lst+=related_lst\n",
        "            final_title_lst+=title_lst\n",
        "            final_url_list+=url_list\n",
        "            final_content_lst+=content_lst\n",
        "\n",
        "    # Handle the case where end_page is less than start_page\n",
        "    else:\n",
        "        print('Error: The end_page is lesser than the start_page')\n",
        "\n",
        "    # Return the final scraped data\n",
        "    return final_date_list, final_related_lst, final_title_lst, final_url_list, final_content_lst\n",
        "\n",
        "def scrape(url_to_scrape, num_results_per_page):\n",
        "    output_filename = 'output.txt'\n",
        "\n",
        "    scraped_data, url_list, date_list = scrape_main_website(url_to_scrape)\n",
        "\n",
        "    if scraped_data:\n",
        "        write_to_file(scraped_data, output_filename)\n",
        "\n",
        "    lst=[]\n",
        "    try:\n",
        "        with open(output_filename, 'r', encoding='utf-8') as read_file:\n",
        "            for line in read_file:\n",
        "                lst.append(line.strip())\n",
        "    except FileNotFoundError:\n",
        "        print(f\"The file '{file_path}' does not exist.\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "    lst=lst[2:len(lst)-3]\n",
        "    title_lst = [lst[i] for i in range(len(lst)) if i%2==0] #List of all titles on the page\n",
        "    related_lst = [lst[i] for i in range(len(lst)) if i%2!=0] #List of the related persons\n",
        "\n",
        "    content_lst = []# List of speech content for the page - for all links\n",
        "    for url in url_list:\n",
        "        scraped_speech_data = scrape_content_website(url)\n",
        "\n",
        "        if scraped_data:\n",
        "            write_to_file(scraped_speech_data, output_filename)\n",
        "\n",
        "        content_str=''\n",
        "        try:\n",
        "            with open(output_filename, 'r', encoding='utf-8') as read_file:\n",
        "                for line in read_file:\n",
        "                    if 'The American Presidency Project' not in line and 'Twitter Facebook' not in line:\n",
        "                        content_str+=line\n",
        "        except FileNotFoundError:\n",
        "            print(f\"The file '{file_path}' does not exist.\")\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred: {e}\")\n",
        "\n",
        "        content_lst.append(content_str)\n",
        "\n",
        "    return date_list[:num_results_per_page], related_lst, title_lst, url_list[:num_results_per_page], content_lst[:num_results_per_page]\n",
        "\n",
        "\n",
        "#User input\n",
        "url = input('Enter the base url of the website:')\n",
        "start_page = int(input('Enter the start page from which you want to extract the data:'))\n",
        "end_page = int(input('Enter the end page till which you want to extract the data:'))\n",
        "date_list, related_lst, title_lst, url_list, content_lst = scraping(start_page, end_page, url)\n",
        "\n",
        "data = {\n",
        "    'Speech Link': url_list,\n",
        "    'Date of Speech': date_list,\n",
        "    'Speech Title': title_lst,\n",
        "    'Related Person': related_lst,\n",
        "    'Speech Content': content_lst\n",
        "}\n",
        "\n",
        "scraped_data_df = pd.DataFrame(data)\n",
        "\n",
        "#Uncomment to store the DataFrame to a CSV file\n",
        "scraped_data_df.to_csv('scraped_data.csv', index=False)\n",
        "files.download('/content/drive/My Drive/scraped_data.csv')\n",
        "\n",
        "#Uncomment to store the DataFrame to a Feather file\n",
        "#A Feather file is more effecient as compared to a CSV file in terms of memory and time\n",
        "#feather_file_path = 'scraped_data.feather'\n",
        "#scraped_data_df.to_feather(feather_file_path)\n",
        "# files.download('/content/drive/My Drive/scraped_data.feather')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMa8EFCgaamDwYGMIRE+yuA",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}