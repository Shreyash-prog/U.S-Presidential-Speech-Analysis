{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMjpfc0kqO385CaMruZmRi4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shreyash-prog/U.S-Presidential-Speech-Analysis/blob/main/Miller%20Center/Miller_Center_Web_Scraping.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ysgy4q-M_zvi"
      },
      "outputs": [],
      "source": [
        "#Code to scrape date from specific page intervals\n",
        "\n",
        "# Import necessary libraries\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import re\n",
        "from datetime import datetime\n",
        "from google.colab import drive\n",
        "from google.colab import files\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Function to capture the title of the speech\n",
        "def capture_text_after_pattern(text, pattern):\n",
        "    # Regular expression pattern to match the specified pattern\n",
        "    pattern_regex = re.compile(pattern)\n",
        "\n",
        "    # Using search() to find the pattern in the text\n",
        "    match = pattern_regex.search(text)\n",
        "\n",
        "    if match:\n",
        "        # Return the text after the pattern\n",
        "        return text[match.end():]\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "# Function to scrape content from individual speech pages\n",
        "def scrape_content_website(url):\n",
        "    # Make a GET request to the provided URL\n",
        "    response = requests.get(url)\n",
        "\n",
        "    # Check if the request was successful (status code 200)\n",
        "    if response.status_code == 200:\n",
        "        # Parse the HTML content\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "        # Extract text from paragraph tags (speech content)\n",
        "        paragraphs = soup.find_all('p')\n",
        "        data = '\\n'.join([p.get_text() for p in paragraphs])\n",
        "\n",
        "        if '\\xa0\\n' in data[:-160]:\n",
        "            try:\n",
        "                occurrences = re.finditer('\\xa0\\n', data)\n",
        "                indices_u = [match.start() for match in occurrences]\n",
        "                #indices_u[1]\n",
        "\n",
        "                occurrences = re.finditer('\\n', data)\n",
        "                indices_n = [match.start() for match in occurrences]\n",
        "                #indices_n[2]\n",
        "\n",
        "                speech_summary = data[indices_n[2]+1:indices_u[1]]\n",
        "\n",
        "                speech = data[indices_u[1]+6:]\n",
        "\n",
        "                # Remove Unicode characters\n",
        "                speech = re.sub(r'[^\\x00-\\x7F]+', '', speech)\n",
        "                speech_summary = re.sub(r'[^\\x00-\\x7F]+', '', speech_summary)\n",
        "                # Remove newline characters\n",
        "                speech = re.sub(r'\\n', '', speech)\n",
        "                speech_summary = re.sub(r'\\n', '', speech_summary)\n",
        "                # Remove other non-printable characters\n",
        "                speech = re.sub(r'[\\x00-\\x08\\x0B\\x0C\\x0E-\\x1F\\x7F]+', '', speech)\n",
        "                speech_summary = re.sub(r'[\\x00-\\x08\\x0B\\x0C\\x0E-\\x1F\\x7F]+', '', speech_summary)\n",
        "            except Exception as e:\n",
        "                speech = data\n",
        "                speech_summary = data\n",
        "\n",
        "            return speech, speech_summary\n",
        "        else:\n",
        "            try:\n",
        "                occurrences = re.finditer('\\n', data)\n",
        "                indices_n = [match.start() for match in occurrences]\n",
        "\n",
        "                speech_summary = data[indices_n[2]+1:indices_n[3]]\n",
        "                speech = data[indices_n[3]+1:]\n",
        "\n",
        "                # Remove Unicode characters\n",
        "                speech = re.sub(r'[^\\x00-\\x7F]+', '', speech)\n",
        "                speech_summary = re.sub(r'[^\\x00-\\x7F]+', '', speech_summary)\n",
        "                # Remove newline characters\n",
        "                speech = re.sub(r'\\n', '', speech)\n",
        "                speech_summary = re.sub(r'\\n', '', speech_summary)\n",
        "                # Remove other non-printable characters\n",
        "                speech = re.sub(r'[\\x00-\\x08\\x0B\\x0C\\x0E-\\x1F\\x7F]+', '', speech)\n",
        "                speech_summary = re.sub(r'[\\x00-\\x08\\x0B\\x0C\\x0E-\\x1F\\x7F]+', '', speech_summary)\n",
        "            except Exception as e:\n",
        "                speech = data\n",
        "                speech_summary = data\n",
        "\n",
        "            return speech, speech_summary\n",
        "\n",
        "    else:\n",
        "        print(f\"Failed to fetch the page. Status code: {response.status_code}\")\n",
        "        return None\n",
        "\n",
        "# Function to scrape the main website for speech links and dates\n",
        "def scrape_main_website(president_name, url, speech_link_lst, speech_date_lst, speech_title_lst, speech_lst, speech_summary_lst, president_lst):\n",
        "\n",
        "    # Make a GET request to the provided URL\n",
        "    response = requests.get(url)\n",
        "\n",
        "    # Check if the request was successful (status code 200)\n",
        "    if response.status_code == 200:\n",
        "        # Parse the HTML content\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "        # Extract URLs from anchor tags (links)\n",
        "        paragraphs = soup.find_all('span')\n",
        "        para_str_list = [str(i) for i in paragraphs]\n",
        "        filtered_para_list = [j for j in para_str_list if '\"field-content\"><a href=' in j]\n",
        "\n",
        "    for k in filtered_para_list:\n",
        "        #Extracting the url of the speech\n",
        "        url_pattern = r'https?://\\S+'\n",
        "        url = re.findall(url_pattern, k)\n",
        "        speech_link_lst.append(url[0])\n",
        "\n",
        "        #Extracting the date of the speech\n",
        "        date_pattern = r'((January|February|March|April|May|June|July|August|September|October|November|December)\\s+\\d{1,2},\\s+\\d{4})'\n",
        "        date_string = re.findall(date_pattern, k)\n",
        "        speech_date_lst.append(date_string[0][0])\n",
        "\n",
        "        #Extracting the title of the speech\n",
        "        captured_text = capture_text_after_pattern(k, date_pattern)\n",
        "        speech_title_lst.append(captured_text[2:-11])\n",
        "\n",
        "    #Iterating over the speech links of each president to extract the speech transcript and summary\n",
        "    for link in speech_link_lst:\n",
        "        speech, speech_summary = scrape_content_website(link[:-1])\n",
        "        speech_lst.append(speech)\n",
        "        speech_summary_lst.append(speech_summary)\n",
        "\n",
        "    president_lst = [president_name] * len(speech_title_lst)\n",
        "\n",
        "    return speech_link_lst, speech_date_lst, speech_title_lst, speech_lst, speech_summary_lst, president_lst\n",
        "\n",
        "def scraping():\n",
        "\n",
        "    president_id_list = [44,45,3,4,141,6,7,8,9,10,11,12,13,14,15,16,18,19,22,20,21,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,8396,30721]\n",
        "    president_name_list = ['George Washington', 'John Adams', 'Thomas Jefferson', 'James Madison', 'James Monroe', 'John Quincy Adams', 'Andrew Jackson', 'Martin Van Buren', 'William Harrison', 'John Tyler', 'James K. Polk', 'Zachary Taylor', 'Millard Fillmore', 'Franklin Pierce', 'James Buchanan', 'Abraham Lincoln', 'Ulysses S. Grant', 'Rutherford B. Hayes', 'Grover Cleveland', 'James A. Garfield', 'Chester A. Arthur', 'Benjamin Harrison', 'William McKinley', 'Theodore Roosevelt', 'William Taft', 'Woodrow Wilson', 'Warren G. Harding', 'Calvin Coolidge', 'Herbert Hoover', 'Franklin D. Roosevelt', 'Harry S. Truman', 'Dwight D. Eisenhower', 'John F. Kennedy', 'Lyndon B. Johnson', 'Richard M. Nixon', 'Gerald Ford', 'Jimmy Carter', 'Ronald Reagan', 'George H. W. Bush', 'Bill Clinton', 'George W. Bush', 'Barack Obama', 'Donald Trump', 'Joe Biden']\n",
        "\n",
        "    speech_link_lst, speech_date_lst, speech_title_lst, speech_lst, speech_summary_lst, president_lst = [], [], [], [], [], []\n",
        "\n",
        "    # Iterating over presidents to extract their speeches one by one\n",
        "    for id in range(len(president_id_list)):\n",
        "        url_to_scrape = f'https://millercenter.org/the-presidency/presidential-speeches?field_president_target_id[{president_id_list[id]}]={president_id_list[id]}'\n",
        "        sl, sd, st, s, ss, p = scrape_main_website(president_name_list[id], url_to_scrape, [], [], [], [], [], [])\n",
        "\n",
        "        speech_link_lst+=sl\n",
        "        speech_date_lst+=sd\n",
        "        speech_title_lst+=st\n",
        "        speech_lst+=s\n",
        "        speech_summary_lst+=ss\n",
        "        president_lst+=p\n",
        "\n",
        "    scraped_df = pd.DataFrame({'Speech Link': speech_link_lst, 'Date of Speech': speech_date_lst, 'Speech Title': speech_title_lst, 'Related Person': president_lst, 'Speech Summary': speech_summary_lst, 'Speech Content': speech_lst})\n",
        "\n",
        "    scraped_df.to_csv('scraped_data_w2.csv', index=False)\n",
        "    files.download('/content/drive/My Drive/scraped_data_w2.csv')\n",
        "\n",
        "    return speech_link_lst, speech_date_lst, speech_title_lst, speech_lst, speech_summary_lst, president_lst, scraped_df"
      ]
    }
  ]
}